#!/usr/bin/env python3.5

import os, collections, sys, shutil, subprocess, re, cp437, hashlib, random, glob, operator, json

def fmt_cmd(s):
  s = s.strip()
  s = re.sub('\n', ' ', s)
  s = re.sub(' +', ' ', s)
  return s

def print_count(l):
  c = collections.Counter(l)
  for name, count in c.most_common():
    print('{0: <5} {1}'.format(count, name))

def die(*args):
  if args:
    print(*args)
  sys.exit(-1)

def slurp(path):
  with open(path, 'r') as f:
    return f.read()

commands = {}

def register(f):
  commands[f.__name__] = f

def replace_prefix(s, prefix, replacement):
  if not s.startswith(prefix):
    die('chomp: {} does not start with {}'.format(s, prefix))
  return replacement + s[len(prefix):]

@register
def unpack():
  problematic = []
  for root, subdirs, files in os.walk('dat/raw'):
    dst_root = replace_prefix(root, 'dat/raw', 'dat/unpacked')
    if not os.path.isdir(dst_root):
      os.makedirs(dst_root)
    for name in files:
      src = os.path.join(root, name)
      dst = os.path.join(dst_root, name)
      _, ext = os.path.splitext(name)
      if ext == '.zip':
        cmd = ['unzip', '-n', src, '-d', dst]
      elif ext == '.rar':
        os.makedirs(dst)
        cmd = ['unrar', 'e','-or', src, dst]
      elif ext == '.lzh' or ext == '.lha':
        cmd = ['lha', 'x', '-w', dst, src]
      elif ext == '.arj':
        cmd = ['arj', 'e', src, dst]
      elif ext == '.tgz':
        cmd = ['tar', 'zxvf', src, '-C', dst]
      else:
        cmd = ['cp', src, dst]
      print('running {}'.format(' '.join(cmd)))
      result = subprocess.call(cmd)
      if result != 0:
        print('{} failed!'.format(' '.join(cmd)))
        problematic.append(src)
  print()
  print('problematic:')
  for src in problematic:
    print(src)

CRLF      = re.compile(b'\r\n'                                   )
SGR       = re.compile(b'\x1B[[]([0-9;]*)m'                      )
CURSOR    = re.compile(b'\x1B[[]([0-9;,]*)(A|B|C|D||H|J|K|M|s|u)')
WHATEVER  = re.compile(b'\x1B[[]([^A-Za-z+-]*[A-Za-z+-])'        )

sgr_counter = collections.Counter()
esc_counter = collections.Counter()

good       = []
bad_width  = []
bad_data   = []
empty      = []
cursor     = []
text       = []
duplicates = {}

def writelines(dst, lines):
  with open(dst, 'w') as f:
    for line in lines:
      f.write(line + '\n')

emit_color = False

def translate(src, dst):
  with open(src, 'rb') as f:
    data = f.read()
  digest = hashlib.sha256(data).digest()
  if digest in duplicates:
    duplicates[digest].append(src)
    return
  else:
    duplicates[digest] = [src]
  s         = ''
  printable = 0
  lowercase = 0
  count     = len(data)
  x         = 0
  i         = 0
  bad       = False
  sgrs      = []
  bold      = 0
  italic    = 0
  underline = 0
  blink     = 0
  bg        = 0
  fg        = 7
  def color():
    if not emit_color:
      return ''
    nonlocal bold, fg, bg
    offset = bold & 0b1
    offset <<= 3
    offset |= fg & 0b111
    offset <<= 1
    offset |= blink & 0b1
    offset <<= 3
    offset |= bg & 0b111
    if offset < 0 or offset > 255:
      die('bad offset:', offset)
    return chr(0x2800 + offset)
  def endline():
    nonlocal s, x, bad, color
    if x > 80:
      print('bad width:', x, src)
      bad_width.append(src)
      bad = True
    s += (color() + ' ') * (80 - x)
    s += color() + '\n'
    x = 0
  while i < count and not bad:
    match = SGR.match(data, i)
    if match:
      payload = match.group(1).decode('utf-8')
      sgrs.append(payload)
      ns = [int(s) for s in payload.split(';') if s]
      for n in ns:
        if n == 0:
          bold = italic = underline = blink = 0
          bg = 0
          fg = 7
        elif n == 1:
          bold = 1
        elif n == 3:
          italic = 0
        elif n == 4:
          underline = 0
        elif n == 5:
          blink = 0
        elif n == 24:
          underline = 0
        elif n >= 30 and n <= 37:
          fg = n - 30
        elif n >= 40 and n <= 47:
          bg = n - 40
      i += len(match.group(0))
      continue
    match = CURSOR.match(data, i)
    if match:
      cursor.append(src)
      bad = True
      i += len(match.group(0))
      continue
    match = WHATEVER.match(data, i)
    if match:
      esc_counter[match.group(1)] += 1
      i += len(match.group(0))
      continue
    c = data[i]
    if c == 0x1B: # ESC
      print('stray escape character:', src)
      bad_data.append(src)
      bad = True
    elif c == 0x9: # TAB
      s += (color() + ' ') * 8
      x += 8
      i += 1
    elif c == 0xA: # LF
      endline()
      i += 1
    elif CRLF.match(data, i): # CRLF
      endline()
      i += 2
    elif c == 0x1A: # EOF
      endline()
      break
    else:
      if c != 0 and c != 32:
        printable += 1
      if c >= 97 and c <= 122:
        lowercase += 1
      s += color() + cp437.characters[c]
      x += 1
      i += 1
  if count == 0:
    empty.append(src)
    bad = True
  if s and s[-1] != '\n':
    endline()
  if bad:
    return
  if printable == 0:
    art_ratio = 1
  else:
    art_ratio = 1 - lowercase / printable
  if art_ratio < 0.50:
    text.append(src)
    return
  good.append(src)
  print('good:', src)
  sgr_counter.update(sgrs)
  with open(dst, 'wt') as f:
    f.write(s)

limit = False

@register
def clean():
  exts = set()
  i = 0
  for root, subdirs, files in os.walk('dat/artpacks'):
    if limit and i > 1000:
      break
    dst_root = replace_prefix(root, 'dat/artpacks', 'dat/clean')
    if not os.path.isdir(dst_root):
      os.makedirs(dst_root)
    for name in files:
      ext = os.path.splitext(name)[1][1:].lower()
      if ext not in {'ans', 'asc', 'ice'}:
        continue
      src = os.path.join(root, name)
      dst = os.path.join(dst_root, name)
      translate(src, dst)
      i += 1

  writelines('good.log',      good)
  writelines('bad-width.log', bad_width)
  writelines('bad-data.log',  bad_data)
  writelines('empty.log',     empty)
  writelines('cursor.log',    cursor)
  writelines('text.log',      text)
  writelines('sgr.log', ['{} {}'.format(s, n) for s, n in sgr_counter.most_common()])
  writelines('esc.log', ['{} {}'.format(e, n) for e, n in esc_counter.most_common()])
  lines = []
  for digest, paths in duplicates.items():
    if len(paths) == 1:
      continue
    for path in duplicates[digest]:
      lines.append(path)
    lines.append('')
  writelines('duplicates.log', lines)

@register
def classify():
  i = 0
  for root, subdirs, files in os.walk('dat/clean'):
    for name in files:
      if name == '.DS_Store':
        continue
      src = os.path.join(root, name)
      match = re.match('^dat/clean/(acid|ice|other)/([0-9]{4})/', src)
      if not match:
        die('no match:', src)
      group = match.group(1)
      year = int(match.group(2))
      ext = os.path.splitext(src)[1][1:].lower()
      dst = 'dat/classified/{}-{}-{}-{}.ans'.format(group, year, ext, i)
      shutil.copyfile(src, dst)
      i += 1

@register
def render():
  for root, subdirs, files in os.walk('dat/classified'):
    for name in files:
      src = os.path.join(root, name)
      dst = replace_prefix(src, 'dat/classified', 'dat/rendered')
      render_file(src, dst)

def render_file(src, dst):
  data = open(src, 'r').read()
  with open(dst, 'wb') as f:
    bold = 0
    fg   = 7
    bg   = 0
    x    = 0
    for i in range(len(data)):
      c = data[i]
      n = ord(c)
      if emit_color and i % 2 == 0:
        offset = n - 0x2800
        if offset > 255 or offset < 0:
          sys.exit('bad offset', offset)
        bg = offset & 0b111
        offset >>= 3
        blink = offset & 0b1
        offset >>= 1
        fg = offset & 0b111
        offset >>= 3
        bold = offset & 0b1
      else:
        if c == '\n':
          f.write(b'\n')
          x = 0
        else:
          if x > 78:
            continue
          if emit_color:
            f.write(b'\x1B[0m')
            if bold:
              f.write(b'\x1B[1m')
            if blink:
              f.write(b'\x1B[5m')
            f.write(b'\x1B' + '[{};{}m'.format(30 + fg, 40 + bg).encode('ascii'))
          if n not in cp437.codepoints:
            n = ord(' ')
          original = cp437.codepoints.index(n)
          if original > 255:
            die('bad original:', original)
          f.write(bytes([original]))
          x += 1

def make_training_set(dst, srcs, target_size):
  random.shuffle(srcs)
  with open(dst, 'wb') as df:
    n = 0
    for src in srcs:
      if n >= target_size:
        break
      with open(src, 'rb') as sf:
        data = sf.read()
        n += len(data)
        df.write(data)

@register
def compile():
  srcs = []
  for root, subdirs, files in os.walk('dat/classified'):
    for name in files:
      srcs.append(os.path.join(root, name))
  make_training_set('dat/compiled/2.txt',   srcs,    2 * meg)
  make_training_set('dat/compiled/4.txt',   srcs,    4 * meg)
  make_training_set('dat/compiled/8.txt',   srcs,    8 * meg)
  make_training_set('dat/compiled/16.txt',  srcs,   16 * meg)
  make_training_set('dat/compiled/32.txt',  srcs,   32 * meg)
  make_training_set('dat/compiled/all.txt', srcs, 1000 * meg)

kib = 1 << 10
meg = 1 << 20

def checkpoint_number(checkpoint):
  base, _ = os.path.splitext(checkpoint)
  return int(base.split('_')[-1])

class Item:
  def __init__(self, path):
    name, _              = os.path.splitext(replace_prefix(path, 'dat/compiled/', ''))
    self.name            = name
    self.size            = os.path.getsize(path)
    self.txt             = os.path.join('dat/compiled',     name + '.txt' )
    self.h5              = os.path.join('dat/preprocessed', name + '.h5'  )
    self.json            = os.path.join('dat/preprocessed', name + '.json')
    self.cv_prefix       = os.path.join('dat/checkpoints',  name          )
    self.checkpoints     = sorted(glob.glob(self.cv_prefix + '*.t7'), key= checkpoint_number)
    self.checkpoint_info = [s[:-3] + '.json' for s in self.checkpoints]
    self.samples         = [
      'dat/samples/{}_{}.txt'.format(name, checkpoint_number(checkpoint))
      for checkpoint in self.checkpoints
    ]
    if self.size < 2 * meg * 1.2:
      self.category   = 'small'
      self.rnn_size   = 256
      self.num_layers = 2
      self.max_epochs = 50
    elif self.size < 8 * meg * 1.2:
      self.category   = 'normal'
      self.rnn_size   = 512
      self.num_layers = 3
      self.max_epochs = 50
    elif self.size < 32 * meg * 1.2:
      self.category   = 'huge'
      self.rnn_size   = 1024
      self.num_layers = 3
      self.max_epochs = 50
    else:
      self.category   = 'gigantic'
      self.rnn_size   = 2048
      self.num_layers = 3
      self.max_epochs = 25

def unsorted():
  return [Item(path) for path in glob.glob('dat/compiled/*')]

def by_size():
  return sorted(unsorted(), key=operator.attrgetter('size'))

@register
def preprocess():
  items = by_size()
  count = len(items)
  for i in range(count):
    item = items[i]
    print('preprocessing {} of {}: {}...'.format(i + 1, count, item.name))
    cmd = fmt_cmd('''
      python lib/torch-rnn/scripts/preprocess.py
      --input_txt   {0.txt}
      --output_h5   {0.h5}
      --output_json {0.json}
    '''.format(item))
    subprocess.check_call(cmd, shell=True)

@register
def train():
  items = by_size()
  count = len(items)
  for i in range(count):
    item = items[i]
    print('training {0.category} rnn on {0.name}'.format(item))
    print('size={0.rnn_size} layers={0.num_layers}'.format(item))
    cmd = fmt_cmd('''
    th train.lua
    -input_h5 ../../{0.h5}
    -input_json ../../{0.json}
    -checkpoint_name ../../{0.cv_prefix}
    -checkpoint_every 1000
    -rnn_size {0.rnn_size}
    -num_layers {0.num_layers}
    -max_epochs {0.max_epochs}
    -seq_length 243
    -gpu 0
    ''').format(item)
    subprocess.check_call(cmd, cwd='lib/torch-rnn', shell=True)

@register
def sample():
  items = by_size()
  count = len(items)
  for i in range(count):
    item = items[i]
    if not item.checkpoints:
      print('no checkpoints for {}...'.format(item.name))
      continue
    for i in range(len(item.checkpoints)):
      checkpoint = item.checkpoints[i]
      sample     = item.samples[i]
      print('sampling {}...'.format(checkpoint))
      cmd = fmt_cmd('''
      th sample.lua
      -checkpoint ../../{}
      -length {}
      -verbose 0
      ''').format(checkpoint, 200 * kib)
      output = subprocess.check_output(cmd, cwd='lib/torch-rnn', shell=True)
      with open('{}.{}'.format(sample, n), 'wb') as f:
        f.write(output)

@register
def info():
  items = by_size()
  for item in items:
    for i in range(len(item.checkpoints)):
      data = json.loads(slurp(item.checkpoint_info[i]))
      val_loss = data['val_loss_history'][-1]
      print(item.checkpoints[i], val_loss)

@register
def rasterize():
  for txt in glob.glob('dat/rasterized/*.txt'):
    base, ext = os.path.splitext(txt)
    ans = base + '.ans'
    render_file(txt, ans)
    cmd = 'lib/ansilove-php/ansilove {}'.format(ans)
    print(cmd)
    subprocess.check_call(cmd, shell=True)

if len(sys.argv) != 2:
  die('usage: main COMMAND')

command = sys.argv[1]

def arg(n):
  if n + 2 < len(sys.argv):
    return sys.argv[n + 2]
  else:
    return None

if command in commands:
  if command[-1] == 'e':
    print('{}ing...'.format(command[0:-1]))
  else:
    print('{}ing...'.format(command))
  commands[command]()
else:
  die('unexpected command:', command)
